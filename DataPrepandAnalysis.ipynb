{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Basic Text Analysis - Topic Modeling Pt. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1. Zotero API + PDF Text Miner\n",
    "We need this so we don't have to bring down all of the pdfs in our library to our local machines, and we can always run our data prep on the most up-to-date corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import ftfy\n",
    "import nltk.corpus\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import glob\n",
    "import requests\n",
    "import urllib.request\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoteroCrawler(key, groupid):\n",
    "    multiplier = 0\n",
    "    zoterogroup = []\n",
    "    # get number of items in group\n",
    "    headers = {\"Zotero-API-Version\":\"3\",'Connection':'close', \"Zotero-API-Key\":key}\n",
    "    checkurl = \"https://api.zotero.org/groups/\" +groupid\n",
    "    rcheck = requests.get(checkurl, headers=headers)\n",
    "    items = rcheck.json()['meta']['numItems']\n",
    "    print(items)\n",
    "    pages = math.ceil((items/100))\n",
    "    while pages > 0:\n",
    "        url = \"https://api.zotero.org/groups/\" +groupid + \"/items\" + \"?limit=100&start=\" + str(multiplier)\n",
    "        print(url)\n",
    "        r = requests.get(url, headers=headers)\n",
    "        rj = r.json() #jsonified version of our Zotero group\n",
    "        print(len(rj))\n",
    "        for i in rj:\n",
    "            zoterogroup.append(i)\n",
    "        multiplier = multiplier + 100\n",
    "        pages = pages - 1\n",
    "    return zoterogroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"***\"\n",
    "group_id = \"2808857\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rj= zoteroCrawler(api_key, group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def download_file(download_url, filename):\n",
    "    response = urllib.request.urlopen(download_url)    \n",
    "    file = open(filename + \".pdf\", 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# now what? get the attachments\n",
    "# inputs are jsonified version of your zotero group, base url to zotero group items, and api key\n",
    "# going to have to loop through and run this for every page I think\n",
    "def attachmentGrabber(rj, url, key):\n",
    "    counter = 0\n",
    "    to_extract = []\n",
    "    citation_list = []\n",
    "    for i in rj:\n",
    "        item_key = rj[counter][\"key\"]\n",
    "        headers = {\"Zotero-API-Version\":\"3\",'Connection':'close', \"Zotero-API-Key\":key}\n",
    "        attach = requests.get(url + item_key + \"/file\", headers=headers)\n",
    "        if \"200\" in str(attach):\n",
    "            download_file(attach.url, f'Document_{item_key}')\n",
    "            to_extract.append(f'Document_{item_key}.pdf')\n",
    "            print(\"GOOD\" + \" 1 \" + str(item_key))\n",
    "        else:\n",
    "            try:\n",
    "                attach = requests.get(rj[counter]['links']['attachment']['href'] + \"/file\", headers=headers)\n",
    "                print(attach)\n",
    "                if \"200\" in str(attach):\n",
    "                    download_file(attach.url, f'Document_{item_key}')\n",
    "                    to_extract.append(f'Document_{item_key}.pdf')\n",
    "                    print(\"GOOD\" + \" \" + str(item_key))\n",
    "                else:\n",
    "                    print(\"that didn't work\" + \" \" + attach.url)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        counter +=1\n",
    "    return to_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://api.zotero.org/groups/2808857/items/\"\n",
    "to_extract  = attachmentGrabber(rj, url, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we extract the PDFs, this is where we should use the Scholarcy API to grab their references. https://ref.scholarcy.com/api/ We should play around with different return types to see what will be easiest. Part of me thinks it will be easiest to isolate references and compare them with the text in the PDF if we use the ```/extract``` endpoint to output in JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scholarcy_headers = {\"accept\":\"application/json\",\"Authorization\":\"Bearer \", \"Content-Type\": 'multipart/form-data','Connection':'close' }\n",
    "alt_headers = {\"accept\":\"application/json\",\"Authorization\":\"Bearer \", \"Content-Type\": 'multipart/form-data',\"engine\": \"v2\",'Connection':'close'}\n",
    "url = \"https://ref.scholarcy.com/api/references/extract\"\n",
    "refdict = {}\n",
    "for i in to_extract:\n",
    "    time.sleep(2)\n",
    "    print(\"Working on \" + i + \"...\")\n",
    "    try:\n",
    "        file = {'file': open(i, 'rb')}\n",
    "        r = requests.post(url, files=file, data=scholarcy_headers)\n",
    "        print(r)\n",
    "        refdict[i] = r.json()\n",
    "        print(i + \" \" + \"is good to go\")\n",
    "    except Exception as e:\n",
    "        print(str(e) + \" \" + \"Trying API v2 instead...\")\n",
    "        try:\n",
    "            r2 = requests.post(url, files=file,data=alt_headers)\n",
    "            print(r2)\n",
    "            refdict[i] = r2.json()\n",
    "            print(i + \" \" + \"is good to go\")\n",
    "        except Exception as e:\n",
    "            print(\"That didn't work either...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attachments have been procured. Let's do something with them. Thanks to [PDF Text Miner](https://github.com/prldc/pdf_text_miner) for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def extract_pdfs(list):  # You can easily extract a list from a .csv with pandas.\n",
    "    d = {'file_name': ['dummy'], 'file_text': ['dummy'], 'ocr': [False]}\n",
    "    df = pd.DataFrame(d, columns=['file_name', 'file_text', 'ocr'])\n",
    "    count = 1\n",
    "    for pdf in list:\n",
    "        try:\n",
    "            ext = os.path.splitext(pdf)[1][1:].strip()  # Gets file extension.\n",
    "            if ext == 'pdf':  # Guarantees that the file is a .pdf, otherwise the program will crash when extracting text.\n",
    "                ocr = False\n",
    "                name = pdf.split('.pdf')[0]\n",
    "                doc = fitz.open(f\"{name}.pdf\")\n",
    "                text_file = open(f\"{name}.txt\", 'w')\n",
    "                number_of_pages = doc.pageCount\n",
    "                for page_n in range(number_of_pages):  # Extracts text from each page.\n",
    "                    page = doc.load_page(page_n)\n",
    "                    page_content = page.get_text(\"text\")\n",
    "                    text_file.write(page_content)\n",
    "                if os.stat(\n",
    "                        f\"{name}.txt\").st_size < 2000:  # Assumes file lacks OCR based on .txt file size, starts Tesseract.\n",
    "                    ocr = True\n",
    "                    os.remove(f\"{name}.txt\")  # Removes the previously scraped .txt.\n",
    "                    tess_file = f\"{name}.pdf\"\n",
    "                    pages = convert_from_path(tess_file, 500)\n",
    "                    image_counter = 1\n",
    "                    for page in pages:  # Converts the PDF to image.\n",
    "                        filename = f\"{name}page_{str(image_counter)}.jpg\"\n",
    "                        page.save(filename, 'JPEG')\n",
    "                        image_counter = image_counter + 1\n",
    "                    filelimit = image_counter - 1\n",
    "                    outfile = f\"{name}.txt\"\n",
    "                    f = open(outfile, \"a\")\n",
    "                    for i in range(1, filelimit + 1):  # Applies OCR to each image, saves text file.\n",
    "                        filename = f\"{name}page_{str(i)}.jpg\"\n",
    "                        text = str((pytesseract.image_to_string(Image.open(filename), lang=\"por\")))\n",
    "                        text = text.replace('-\\n', '')\n",
    "                        f.write(text)\n",
    "                    f.close()\n",
    "                text = open(f\"{name}.txt\", 'r')\n",
    "                txt = \" \".join(text.readlines())\n",
    "                df = df.append({'file_name': f\"{name}\", 'file_text': txt, 'ocr': ocr}, ignore_index=True)    \n",
    "                end = datetime.datetime.now()\n",
    "                print(\n",
    "                    f\"Finished {name} at {end}. OCR = {ocr}. {count} files read. {round(count * 100 / len(list), 2)}% done.\")\n",
    "        except:\n",
    "            print(f'Did not finish {pdf}... check out that one.')\n",
    "        count = count + 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = extract_pdfs(to_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out # we are able to scrape the vast majority of the articles without a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for rapid topic modeling/cleaning prototyping:\n",
    "out.to_csv(\"extracted_text.csv\")\n",
    "with open('refs.txt', 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(refdict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2. Cleaning up the text\n",
    "Using strategies based on [this article](https://monkeylearn.com/blog/text-cleaning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, before we do any other cleaning tasks, this is where we need to match and remove references with the information we got from the Scholarcy API above.\n",
    "\n",
    "#### Resources:\n",
    "- [String comparison in Python](https://note.nkmk.me/en/python-str-compare/)\n",
    "- [Potentially useful example on StackOverflow](https://stackoverflow.com/questions/39551029/if-else-statement-for-finding-the-index-of-a-character-in-a-string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def referenceChecker(extracted_df):\n",
    "    ref_excluded = {}\n",
    "    counter = 0\n",
    "    string3=\"\"\n",
    "    matches=0\n",
    "    nomatch=0\n",
    "    for name in extracted_df['file_name']:\n",
    "        if \"dummy\" not in name:\n",
    "            text_file = open(str(name) +\".txt\", \"r\")\n",
    "            data = text_file.read()      # Read whole file to a string\n",
    "            text_file.close()         # Close file\n",
    "            string1 = data.replace('\\n',\"\")\n",
    "            string2 = string1.replace('\\t',\"\")\n",
    "            for i in refdict[str(name) + \".pdf\"][\"references\"]:\n",
    "                if i[0:24] in string2:\n",
    "                    starti =string2.find(i[0:24])\n",
    "                    endi = int(starti) + len(i)\n",
    "                    matches+=1\n",
    "                    string3 = string2.replace(string2[starti:endi], \"\")\n",
    "                else:\n",
    "                    pass\n",
    "                    nomatch +=1\n",
    "            ref_excluded[name] = string3\n",
    "    print(\"Matches: \" + str(matches) + \" No Matches: \" +str(nomatch) )\n",
    "    return ref_excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ex = referenceChecker(out) #needs some work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Tasks\n",
    "- Case Normalization\n",
    "- Remove Unicode Characters\n",
    "    - In the future, we may want to experiment with using [ftfy](https://github.com/rspeer/python-ftfy), which fixes text encoding issues, in this pipeline. We may also be interested in exploring [scrubadub](https://scrubadub.readthedocs.io/en/stable/index.html), which redacts potential PII from text.\n",
    "- Remove Stopwords\n",
    "- Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textCleaner(ref_ex_dict):\n",
    "    full_corpus = []\n",
    "    for i in ref_ex_dict.keys():\n",
    "        data = ref_ex_dict[i]\n",
    "        da = data.lower()\n",
    "        d = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", da)\n",
    "        stop = stopwords.words('english')\n",
    "        all_words = nltk.word_tokenize(d)\n",
    "        words = [w for w in all_words if w not in stop]\n",
    "        words = [w for w in words if w.isalpha()]\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        word_out = []\n",
    "        for word in words:\n",
    "            a = lemmatizer.lemmatize(word)\n",
    "            word_out.append(a)\n",
    "        ref_ex_dict[i] = word_out\n",
    "        full_corpus.append(word_out)\n",
    "    return ref_ex_dict, full_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = textCleaner(ref_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusdict = a[0]\n",
    "corpus = a[1]\n",
    "full_corpus = [x for xs in corpus for x in xs] #flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick frequency distribution of the *most common words* in the corpus, and the *most common two and three word collocations* in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fd = nltk.FreqDist(full_corpus)\n",
    "word_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_fd = nltk.FreqDist(nltk.bigrams(full_corpus))\n",
    "bigram_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_fd = nltk.FreqDist(nltk.trigrams(full_corpus))\n",
    "trigram_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using some of nltk's built in functions to get more information about the collocation scores according to association measures.\n",
    "See more information about the nltk collocation methodology [here](https://www.nltk.org/api/nltk.collocations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder(word_fd, bigram_fd)\n",
    "finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional File Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for f in glob.glob(\"Document_*\"):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
